name: Deploy Karpenter and HPA

on:
  push:
    branches: [ main ]
  workflow_dispatch:

env:
  AWS_REGION: us-west-2
  CLUSTER_NAME: wiary
  KARPENTER_VERSION: 0.16.3
  KUBECONFIG: /home/runner/.kube/config

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v1
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Install kubectl
      run: |
        curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
        chmod +x kubectl
        sudo mv kubectl /usr/local/bin/

    - name: Install AWS CLI
      run: |
        curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
        unzip awscliv2.zip
        sudo ./aws/install --update

    - name: Configure AWS CLI
      run: |
        aws configure set aws_access_key_id ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws configure set aws_secret_access_key ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws configure set region ${{ env.AWS_REGION }}

    - name: Setup kubeconfig
      env:
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        AWS_DEFAULT_REGION: ${{ env.AWS_REGION }}
        KUBECONFIG: ${{ env.KUBECONFIG }}
      run: |
        echo "Creating .kube directory..."
        mkdir -p /home/runner/.kube
        echo "Getting cluster endpoint..."
        CLUSTER_ENDPOINT=$(aws eks describe-cluster --name ${{ env.CLUSTER_NAME }} --query "cluster.endpoint" --output text)
        echo "Cluster endpoint: $CLUSTER_ENDPOINT"
        echo "Getting certificate data..."
        CERTIFICATE_DATA=$(aws eks describe-cluster --name ${{ env.CLUSTER_NAME }} --query "cluster.certificateAuthority.data" --output text)
        echo "Creating kubeconfig file..."
        cat > ${{ env.KUBECONFIG }} << 'EOF'
        apiVersion: v1
        kind: Config
        clusters:
        - cluster:
            server: ${CLUSTER_ENDPOINT}
            certificate-authority-data: ${CERTIFICATE_DATA}
          name: ${CLUSTER_NAME}
        contexts:
        - context:
            cluster: ${CLUSTER_NAME}
            user: aws
          name: aws
        current-context: aws
        preferences: {}
        users:
        - name: aws
          user:
            exec:
              apiVersion: client.authentication.k8s.io/v1beta1
              command: aws
              args:
                - eks
                - get-token
                - --cluster-name
                - ${CLUSTER_NAME}
              env:
                - name: AWS_ACCESS_KEY_ID
                  value: ${AWS_ACCESS_KEY_ID}
                - name: AWS_SECRET_ACCESS_KEY
                  value: ${AWS_SECRET_ACCESS_KEY}
                - name: AWS_DEFAULT_REGION
                  value: ${AWS_DEFAULT_REGION}
              interactiveMode: Never
              provideClusterInfo: false
        EOF
        echo "Setting kubeconfig permissions..."
        chmod 600 ${{ env.KUBECONFIG }}
        echo "Verifying kubeconfig file exists..."
        ls -la ${{ env.KUBECONFIG }}
        echo "Kubeconfig file contents:"
        cat ${{ env.KUBECONFIG }}
        echo "Replacing variables in kubeconfig..."
        sed -i "s|\${CLUSTER_ENDPOINT}|$CLUSTER_ENDPOINT|g" ${{ env.KUBECONFIG }}
        sed -i "s|\${CERTIFICATE_DATA}|$CERTIFICATE_DATA|g" ${{ env.KUBECONFIG }}
        sed -i "s|\${CLUSTER_NAME}|${{ env.CLUSTER_NAME }}|g" ${{ env.KUBECONFIG }}
        sed -i "s|\${AWS_ACCESS_KEY_ID}|${{ secrets.AWS_ACCESS_KEY_ID }}|g" ${{ env.KUBECONFIG }}
        sed -i "s|\${AWS_SECRET_ACCESS_KEY}|${{ secrets.AWS_SECRET_ACCESS_KEY }}|g" ${{ env.KUBECONFIG }}
        sed -i "s|\${AWS_DEFAULT_REGION}|${{ env.AWS_REGION }}|g" ${{ env.KUBECONFIG }}
        echo "Final kubeconfig contents:"
        cat ${{ env.KUBECONFIG }}

    - name: Verify cluster access
      env:
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        AWS_DEFAULT_REGION: ${{ env.AWS_REGION }}
        KUBECONFIG: ${{ env.KUBECONFIG }}
      run: |
        echo "Current directory:"
        pwd
        echo "Home directory:"
        echo $HOME
        echo "Kubeconfig path:"
        echo ${{ env.KUBECONFIG }}
        echo "Kubeconfig exists:"
        ls -la ${{ env.KUBECONFIG }}
        echo "Testing AWS credentials..."
        aws sts get-caller-identity
        echo "Getting EKS token..."
        aws eks get-token --cluster-name ${{ env.CLUSTER_NAME }}
        echo "Setting AWS credentials in environment..."
        export AWS_ACCESS_KEY_ID=${{ secrets.AWS_ACCESS_KEY_ID }}
        export AWS_SECRET_ACCESS_KEY=${{ secrets.AWS_SECRET_ACCESS_KEY }}
        export AWS_DEFAULT_REGION=${{ env.AWS_REGION }}
        echo "Testing kubectl with AWS CLI..."
        aws eks get-token --cluster-name ${{ env.CLUSTER_NAME }} | kubectl --kubeconfig=${{ env.KUBECONFIG }} get nodes --token=$(aws eks get-token --cluster-name ${{ env.CLUSTER_NAME }} --query 'status.token' --output text)

    - name: Install Karpenter
      env:
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        AWS_DEFAULT_REGION: ${{ env.AWS_REGION }}
        KUBECONFIG: ${{ env.KUBECONFIG }}
      run: |
        helm repo add karpenter https://charts.karpenter.sh
        helm repo update
        helm install karpenter karpenter/karpenter \
          --namespace karpenter \
          --create-namespace \
          --version ${{ env.KARPENTER_VERSION }} \
          --set controller.clusterName=${{ env.CLUSTER_NAME }} \
          --set controller.clusterEndpoint=$(aws eks describe-cluster --name ${{ env.CLUSTER_NAME }} --query "cluster.endpoint" --output text) \
          --set controller.aws.defaultInstanceProfile=KarpenterNodeInstanceProfile \
          --set controller.aws.useServiceLinkedRole=true \
          --set controller.podAnnotations."iam\.amazonaws\.com/role=KarpenterNodeInstanceRole" \
          --set controller.env[0].name=AWS_REGION \
          --set controller.env[0].value=${{ env.AWS_REGION }} \
          --set controller.env[1].name=AWS_DEFAULT_REGION \
          --set controller.env[1].value=${{ env.AWS_REGION }} \
          --set controller.env[2].name=CLUSTER_NAME \
          --set controller.env[2].value=${{ env.CLUSTER_NAME }} \
          --set controller.env[3].name=CLUSTER_ENDPOINT \
          --set controller.env[3].value=$(aws eks describe-cluster --name ${{ env.CLUSTER_NAME }} --query "cluster.endpoint" --output text)

    - name: Apply Provisioner
      env:
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        AWS_DEFAULT_REGION: ${{ env.AWS_REGION }}
        KUBECONFIG: ${{ env.KUBECONFIG }}
      run: |
        kubectl --kubeconfig=${{ env.KUBECONFIG }} apply -f provisioner/default-provisioner.yaml -n karpenter

    - name: Apply HPA
      env:
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        AWS_DEFAULT_REGION: ${{ env.AWS_REGION }}
        KUBECONFIG: ${{ env.KUBECONFIG }}
      run: |
        kubectl --kubeconfig=${{ env.KUBECONFIG }} apply -f hpa/ -n karpenter

    - name: Update ASG
      run: |
        aws autoscaling update-auto-scaling-group \
          --auto-scaling-group-name ${{ secrets.ASG_NAME }} \
          --min-size 1 \
          --max-size 1 \
          --desired-capacity 1